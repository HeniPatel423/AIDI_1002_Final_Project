{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: AIDI 1002 Final Term Project Report\n",
    "\n",
    "### Heni Rakshitkumar Patel(200544300)\n",
    "\n",
    "####  Emails: 200544300@student.georgianc.on.ca, patelheni423@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "## A brief overview of swarm intelligence-based algorithms for numerical association rule mining\n",
    "\n",
    "The world is highly competitive, and Artificial Intelligence (AI) is changing how we make decisions in various areas of life, from ads to self-driving cars. AI relies heavily on data, similar to how a car needs oil to run. The decisions made by AI are mostly based on past data from specific domains. This data comes in different forms, such as unstructured, semi-structured, or structured. The section mentions that AI has been evolving, and there are discussions about the potential superintelligence of machines.\n",
    "\n",
    "Researchers in the field of Machine Learning (ML) explore different methods under its umbrella. One crucial area is Association Rule Mining (ARM), which finds relationships between attributes in structured databases. This process helps in market basket analysis, building intelligent systems, and creating rule-based classifiers. Despite a peak in interest around 2014, ARM remains popular among researchers, and new methods continue to emerge each year.\n",
    "\n",
    "### Problem Description:\n",
    "\n",
    "The paper focuses on a specific problem called Numerical Association Rule Mining (NARM). Unlike traditional ARM, NARM deals with datasets containing both numerical and categorical attributes without needing to convert numerical values into categories (discretization). The challenge lies in developing efficient algorithms to find patterns and relationships in these mixed datasets. NARM algorithms are often complex, using nature-inspired techniques, like Evolutionary Algorithms (EAs) and Swarm Intelligence (SI). The complexity arises from a larger search space, and algorithms need to be tailored to handle this effectively.\n",
    "\n",
    "\n",
    "### Context of the Problem:\n",
    "\n",
    "The context of the problem is in the wider world of AI, ML, and ARM. Decision-making increasingly relies on data stored in structured databases, where ARM plays a crucial role. The paper highlights the historical trend of interest in ARM, with a peak around 2014. It introduces NARM as an extension of ARM, specifically designed for datasets with both numerical and categorical attributes. The challenge with NARM is its complexity due to a larger search space, leading to the preference for specialized algorithms based on nature-inspired paradigms like SI. The context suggests a need for tailored algorithms to efficiently address the complexities of NARM.\n",
    "\n",
    "\n",
    "### Limitation About other Approaches:\n",
    "\n",
    "The following limitations are identified regarding existing approaches in Numerical Association Rule Mining (NARM) with Swarm Intelligence (SI) algorithms:\n",
    "\n",
    "Underrepresentation in Literature: The study acknowledges that the field of Numerical Association Rule Mining (NARM) utilizing Swarm Intelligence (SI) algorithms is relatively underexplored in the existing literature. The limited number of methods discussed in peer-reviewed literature, some even in non-English languages, indicates an underestimation of this research area within the academic community.\n",
    "\n",
    "Limited Practical Implementation: While there is a growing body of literature on SI-based algorithms for NARM, practical implementations in the real-world are lacking. The gap between theoretical development and real-world applications remains a limitation, suggesting that these methods have not yet transitioned effectively into practical scenarios.\n",
    "\n",
    "Sparse Comparison of Algorithms: The vastness of the SI-based family of algorithms is acknowledged, but the study points out a lack of comprehensive comparisons between different algorithms in the context of solving NARM problems. Understanding how various algorithms perform in this specific domain is crucial for informed decision-making.\n",
    "\n",
    "Insufficient Theoretical Analysis: The paper highlights a deficiency in theoretical analysis supporting the effectiveness of SI-based algorithms for NARM. A more robust theoretical foundation is needed to demonstrate the suitability and efficiency of these algorithms in addressing the complexities of NARM.\n",
    "\n",
    "\n",
    "\n",
    "### Solution:\n",
    "\n",
    "Addressing the identified limitations involves the following solutions:\n",
    "\n",
    "Encourage Research Efforts: Given the underrepresentation of SI-based algorithms for NARM in the literature, there is a call for increased research efforts in this area. Encouraging researchers to explore and contribute to the development of algorithms and methodologies could help uncover more potential in solving NARM challenges.\n",
    "\n",
    "Bridge the Gap to Real-world Implementation: To address the gap between theoretical advancements and practical implementation, there is a need for initiatives that facilitate the application of SI-based algorithms for NARM in real-world scenarios. Developing frameworks and providing accessible source code for algorithms could promote the adoption of these approaches in practical applications.\n",
    "\n",
    "Conduct Comprehensive Algorithm Comparisons: Conducting thorough comparisons between different SI-based algorithms for NARM is crucial. This would provide insights into the strengths and weaknesses of each algorithm, helping researchers and practitioners make informed choices based on their specific needs and requirements.\n",
    "\n",
    "Enhance Theoretical Analysis: To strengthen the theoretical foundation of SI-based algorithms for NARM, there is a suggestion to invest in more in-depth theoretical analyses. This could involve investigating the underlying principles and mechanisms that make these algorithms effective in handling numerical association rule mining challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Explain the related work using the following table\n",
    "\n",
    "| Reference | Explanation | Dataset/Input |  Future Improvement |\n",
    "| --------- | ------------ | ------------- | ------------------------------- |\n",
    "| [1]       | Differential evolution for association rule mining using categorical and numerical attributes. | Not specified. | testing the algorithm on bigger transaction databases with more features as well as applying this approach to other population-based nature-inspired algorithms |\n",
    "| [2]       | Improved Nature-Inspired Algorithms for Numeric Association Rule Mining. | Not specified. | Not specified. |\n",
    "| [3]       | A brief overview of swarm intelligence-based algorithms for numerical association rule mining. | Not specified. | Not specified. |\n",
    "| [4]       | Visualization of Numerical Association Rules by Hill Slopes. | Not specified. | In the future, the method could also be broadened for dealing with mixed attributes, i.e., numerical and categorical. The method should be applied to another transaction databases. |\n",
    "| [5]       | Population-based metaheuristics for Association Rule Text Mining. | Proceedings of the 2020 4th International Conference on Intelligent Systems, Metaheuristics & Swarm Intelligence. |  the evaluation of this method on the other well-known transaction databases could also represent a big challenge for the future.\n",
    " |\n",
    "| [6]       | Data squashing as preprocessing in association rule mining. | 2022 IEEE Symposium Series on Computational Intelligence (SSCI). | , the automatic setting of the threshold parameter, which has a crucial impact on the results, will be studied. A deeper insight into the parameter setting could be obtained by analysing the other, especially large-sized, UCI ML datasets. |\n",
    "| [7]       | A brief overview of swarm intelligence-based algorithms for numerical association rule mining. | Abalone data |There is also a lack of theoretical analysis that should work in favor of SI-based algorithms for NARM.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "\n",
    "To implement the numerical association rule mining on the both categorical and numerical data here NiaARM is used for the implementation of the concept.\n",
    "\n",
    "## NiaARM - A minimalistic framework for Numerical Association Rule Mining  \n",
    "\n",
    "NiaARM is a framework for Association Rule Mining based on nature-inspired algorithms for optimization. The framework is written fully in Python and runs on all platforms. NiaARM allows users to preprocess the data in a transaction database automatically, to search for association rules and provide a pretty output of the rules found. This framework also supports integral and real-valued types of attributes besides the categorical ones. Mining the association rules is defined as an optimization problem, and solved using the nature-inspired algorithms that come from the related framework called NiaPy.  \n",
    "\n",
    "\n",
    "![NiaARM](datasets/logo.png \"Title of the figure, location is simply the directory of the notebook\")\n",
    "\n",
    "\n",
    "#### Detailed insights\n",
    "\n",
    "The current version includes the following functions:\n",
    "\n",
    "-loading datasets in CSV format,\n",
    "\n",
    "-preprocessing of data,\n",
    "\n",
    "-searching for association rules,\n",
    "\n",
    "-providing output of mined association rules,\n",
    "\n",
    "-generating statistics about mined association rules,\n",
    "\n",
    "-visualization of association rules,\n",
    "\n",
    "-association rule text mining (experimental).\n",
    "\n",
    "Implementation was done on the 'Abalone.csv' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution to Project\n",
    "\n",
    "## A new experiment has been conducted using the 'heart.csv' dataset.\n",
    "\n",
    "\n",
    "Dataset link: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
    "\n",
    "\n",
    "#### This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V.\n",
    "#### It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. \n",
    "#### The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. age\n",
    "2. sex\n",
    "3. chest pain type (4 values)\n",
    "4. resting blood pressure\n",
    "5. serum cholestoral in mg/dl\n",
    "6. fasting blood sugar > 120 mg/dl\n",
    "7. resting electrocardiographic results (values 0,1,2)\n",
    "8. maximum heart rate achieved\n",
    "9. exercise induced angina\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. the slope of the peak exercise ST segment\n",
    "12. number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n",
    "\n",
    "The names and social security numbers of the patients were recently removed from the database, replaced with dummy values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install NiaARM with pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pip install niaarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "#### In NiaARM, data loading is done via the Dataset class. There are two options for loading data:\n",
    "\n",
    "Option 1: From a pandas DataFrame (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET INFO:\n",
      "Number of transactions: 1025\n",
      "Number of features: 14\n",
      "\n",
      "FEATURE INFO:\n",
      "\n",
      "            age  sex   cp trestbps chol  fbs restecg thalach exang oldpeak slope   ca thal target\n",
      "dtype       int  int  int      int  int  int     int     int   int   float   int  int  int    int\n",
      "min_val      29    0    0       94  126    0       0      71     0     0.0     0    0    0      0\n",
      "max_val      77    1    3      200  564    1       2     202     1     6.2     2    4    3      1\n",
      "categories  N/A  N/A  N/A      N/A  N/A  N/A     N/A     N/A   N/A     N/A   N/A  N/A  N/A    N/A\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from niaarm import Dataset\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/heart.csv')\n",
    "# preprocess data...\n",
    "data = Dataset(df)\n",
    "print(data) # printing the dataset will generate a feature report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: From CSV file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET INFO:\n",
      "Number of transactions: 1025\n",
      "Number of features: 14\n",
      "\n",
      "FEATURE INFO:\n",
      "\n",
      "            age  sex   cp trestbps chol  fbs restecg thalach exang oldpeak slope   ca thal target\n",
      "dtype       int  int  int      int  int  int     int     int   int   float   int  int  int    int\n",
      "min_val      29    0    0       94  126    0       0      71     0     0.0     0    0    0      0\n",
      "max_val      77    1    3      200  564    1       2     202     1     6.2     2    4    3      1\n",
      "categories  N/A  N/A  N/A      N/A  N/A  N/A     N/A     N/A   N/A     N/A   N/A  N/A  N/A    N/A\n"
     ]
    }
   ],
   "source": [
    "from niaarm import Dataset\n",
    "\n",
    "\n",
    "data = Dataset('datasets/heart.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "#### Data Squashing\n",
    "Optionally, a preprocessing technique, called data squashing [5], can be applied. This will significantly reduce the number of transactions, while providing similar results to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET INFO:\n",
      "Number of transactions: 4200\n",
      "Number of features: 15\n",
      "\n",
      "FEATURE INFO:\n",
      "\n",
      "              age    sex     cp trestbps   chol    fbs restecg thalach  exang oldpeak  slope     ca   thal target      0\n",
      "dtype       float  float  float    float  float  float   float   float  float   float  float  float  float  float  float\n",
      "min_val       N/A    N/A    N/A      N/A    N/A    N/A     N/A     N/A    N/A     N/A    N/A    N/A    N/A    N/A    0.0\n",
      "max_val       N/A    N/A    N/A      N/A    N/A    N/A     N/A     N/A    N/A     N/A    N/A    N/A    N/A    N/A  564.0\n",
      "categories    N/A    N/A    N/A      N/A    N/A    N/A     N/A     N/A    N/A     N/A    N/A    N/A    N/A    N/A    N/A\n"
     ]
    }
   ],
   "source": [
    "from niaarm import Dataset, squash\n",
    "\n",
    "dataset = Dataset('datasets/heart.csv')\n",
    "squashed = squash(dataset, threshold=0.9, similarity='euclidean')\n",
    "print(squashed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[age([29, 77]), sex([0, 1]), cp([0, 3]), trestbps([94, 200]), chol([126, 564]), fbs([0, 1]), restecg([0, 2]), thalach([71, 202]), exang([0, 1]), oldpeak([0.0, 6.2]), slope([0, 2]), ca([0, 4]), thal([0, 3])] => [target([0, 1])]\n",
      "Support: 1.0\n",
      "Confidence: 1.0\n",
      "Lift: 1.0\n"
     ]
    }
   ],
   "source": [
    "from niaarm import Dataset, Feature, Rule\n",
    "\n",
    "# load the heart dataset\n",
    "data = Dataset(\"datasets/heart.csv\")\n",
    "\n",
    "# making the rule All Features => Target([0, 1]) for our heart data\n",
    "antecedent = [\n",
    "    Feature(\"age\", dtype=\"int\", min_val=29, max_val=77),\n",
    "    Feature(\"sex\", dtype=\"int\", min_val=0, max_val=1),\n",
    "    Feature(\"cp\", dtype=\"int\", min_val=0, max_val=3),\n",
    "    Feature(\"trestbps\", dtype=\"int\", min_val=94, max_val=200),\n",
    "    Feature(\"chol\", dtype=\"int\", min_val=126, max_val=564),\n",
    "    Feature(\"fbs\", dtype=\"int\", min_val=0, max_val=1),\n",
    "    Feature(\"restecg\", dtype=\"int\", min_val=0, max_val=2),\n",
    "    Feature(\"thalach\", dtype=\"int\", min_val=71, max_val=202),\n",
    "    Feature(\"exang\", dtype=\"int\", min_val=0, max_val=1),\n",
    "    Feature(\"oldpeak\", dtype=\"float\", min_val=0.0, max_val=6.2),\n",
    "    Feature(\"slope\", dtype=\"int\", min_val=0, max_val=2),\n",
    "    Feature(\"ca\",  dtype=\"int\", min_val=0, max_val=4),\n",
    "    Feature(\"thal\", dtype=\"int\", min_val=0, max_val=3)\n",
    "]\n",
    "consequent = [Feature(\"target\", dtype=\"int\", min_val=0, max_val=1)]\n",
    "\n",
    "# pass the transaction data to the Rule constructor to enable the calculation of metrics\n",
    "rule = Rule(antecedent, consequent, transactions=data.transactions)\n",
    "\n",
    "print(rule)\n",
    "print(f\"Support: {rule.support}\")\n",
    "print(f\"Confidence: {rule.confidence}\")\n",
    "print(f\"Lift: {rule.lift}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining association rules\n",
    "\n",
    "#### Association rule mining can be easily performed using the get_rules function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness: 0.12793586269196025, Support: 0.024390243902439025, Confidence: 0.23148148148148148\n",
      "Fitness: 0.3581833473507149, Support: 0.08878048780487804, Confidence: 0.6275862068965518\n",
      "Fitness: 0.5121951219512195, Support: 0.024390243902439025, Confidence: 1.0\n",
      "Fitness: 0.5229268292682927, Support: 0.045853658536585365, Confidence: 1.0\n",
      "Fitness: 0.6123756481659305, Support: 0.4058536585365854, Confidence: 0.8188976377952756\n",
      "Fitness: 0.6917073170731707, Support: 0.6917073170731707, Confidence: 0.6917073170731707\n",
      "Fitness: 0.7764387204619709, Support: 0.5980487804878049, Confidence: 0.9548286604361371\n",
      "Fitness: 0.8223222431725564, Support: 0.7219512195121951, Confidence: 0.9226932668329177\n",
      "Fitness: 0.9101231586573292, Support: 0.9034146341463415, Confidence: 0.9168316831683169\n",
      "Fitness: 0.9135053556792809, Support: 0.8351219512195122, Confidence: 0.9918887601390498\n",
      "Fitness: 0.9824390243902439, Support: 0.9824390243902439, Confidence: 0.9824390243902439\n",
      "Fitness: 1.0, Support: 1.0, Confidence: 1.0\n",
      "STATS:\n",
      "Total rules: 913\n",
      "Average fitness: 0.3628234269639607\n",
      "Average support: 0.17271284695322187\n",
      "Average confidence: 0.5529340069746985\n",
      "Average lift: 1.1200145239974721\n",
      "Average coverage: 0.3736734966473429\n",
      "Average consequent support: 0.537379317714316\n",
      "Average conviction: 51502709600747.15\n",
      "Average amplitude: 0.41048358772169086\n",
      "Average inclusion: 0.42004381161007553\n",
      "Average interestingness: 0.17559740977784058\n",
      "Average comprehensibility: 0.606679276034527\n",
      "Average netconf: 0.014664958544927734\n",
      "Average Yule's Q: 0.12095757234973327\n",
      "Average antecedent length: 3.4238773274917853\n",
      "Average consequent length: 2.456736035049288\n",
      "\n",
      "Run Time: 3.263734499999998\n",
      "Rules exported to output.csv\n"
     ]
    }
   ],
   "source": [
    "from niaarm import Dataset, get_rules\n",
    "from niapy.algorithms.basic import DifferentialEvolution\n",
    "\n",
    "data = Dataset(\"datasets/Heart.csv\")\n",
    "\n",
    "algo = DifferentialEvolution(population_size=50, differential_weight=0.5, crossover_probability=0.9)\n",
    "metrics = ('support', 'confidence')\n",
    "\n",
    "rules, run_time = get_rules(data, algo, metrics, max_iters=30, logging=True)\n",
    "\n",
    "print(rules) # Prints basic stats about the mined rules\n",
    "print(f'Run Time: {run_time}')\n",
    "rules.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent</th>\n",
       "      <th>consequent</th>\n",
       "      <th>fitness</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>coverage</th>\n",
       "      <th>rhs_support</th>\n",
       "      <th>conviction</th>\n",
       "      <th>amplitude</th>\n",
       "      <th>inclusion</th>\n",
       "      <th>interestingness</th>\n",
       "      <th>comprehensibility</th>\n",
       "      <th>netconf</th>\n",
       "      <th>yulesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[fbs([0, 1])]</td>\n",
       "      <td>[sex([0, 1])]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[sex([0, 1]), thal([0, 3]), ca([0, 4])]</td>\n",
       "      <td>[target([0, 1])]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sex([0, 1])]</td>\n",
       "      <td>[target([0, 1])]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[fbs([0, 1])]</td>\n",
       "      <td>[sex([0, 1]), target([0, 1])]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>0.792481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[fbs([0, 1]), target([0, 1]), thal([0, 3])]</td>\n",
       "      <td>[exang([0, 1])]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    antecedent                     consequent  \\\n",
       "0                                [fbs([0, 1])]                  [sex([0, 1])]   \n",
       "1      [sex([0, 1]), thal([0, 3]), ca([0, 4])]               [target([0, 1])]   \n",
       "2                                [sex([0, 1])]               [target([0, 1])]   \n",
       "3                                [fbs([0, 1])]  [sex([0, 1]), target([0, 1])]   \n",
       "4  [fbs([0, 1]), target([0, 1]), thal([0, 3])]                [exang([0, 1])]   \n",
       "\n",
       "   fitness  support  confidence  lift  coverage  rhs_support  conviction  \\\n",
       "0      1.0      1.0         1.0   1.0       1.0          1.0         0.0   \n",
       "1      1.0      1.0         1.0   1.0       1.0          1.0         0.0   \n",
       "2      1.0      1.0         1.0   1.0       1.0          1.0         0.0   \n",
       "3      1.0      1.0         1.0   1.0       1.0          1.0         0.0   \n",
       "4      1.0      1.0         1.0   1.0       1.0          1.0         0.0   \n",
       "\n",
       "   amplitude  inclusion  interestingness  comprehensibility  netconf  yulesq  \n",
       "0        0.0   0.142857         0.999024           0.630930      0.0     0.0  \n",
       "1        0.0   0.285714         0.999024           0.430677      0.0     0.0  \n",
       "2        0.0   0.142857         0.999024           0.630930      0.0     0.0  \n",
       "3        0.0   0.214286         0.999024           0.792481      0.0     0.0  \n",
       "4        0.0   0.285714         0.999024           0.430677      0.0     0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.read_csv(\"output.csv\")\n",
    "\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "#### The framework currently supports the hill slopes visualization method presented in [4]. More visualization methods are planned to be implemented in future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from niaarm import Dataset, get_rules\n",
    "from niaarm.visualize import hill_slopes\n",
    "\n",
    "dataset = Dataset('datasets/heart.csv')\n",
    "metrics = ('support', 'confidence')\n",
    "rules, _ = get_rules(dataset, 'DifferentialEvolution', metrics, max_evals=1000, seed=1234)\n",
    "some_rule = rules[150]\n",
    "hill_slopes(some_rule, dataset.transactions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "In conclusion, the experiment provides insight into how researchers utilize sophisticated algorithms to uncover patterns and relationships in data containing both numerical and categorical elements. The emphasis is on the advantage of managing numerical values without converting them into categories, a common practice in alternative methods. The experiment explores how scientists modify two crucial components of these algorithms to optimize their performance for this specific task. Many of the algorithms under discussion employ a particular method of representing information and aim to achieve multiple objectives simultaneously.\n",
    "\n",
    "The experiment addresses questions about the significance of this approach, how it undergoes modification, when and where it proves useful, and who primarily engages in this research. \n",
    "\n",
    "### Future Directions:\n",
    "Looking ahead, the experiment suggests several avenues for further exploration in this area. First, researchers are encouraged to delve deeper into this topic, conducting more in-depth studies and experimenting with novel ideas. Second, there is an emphasis on finding ways to apply these methods in real-world situations, making them more practical and impactful. Third, the recommendation is to conduct more thorough comparisons between different methods to determine which ones are most effective. Fourth, there is a call for a deeper understanding of why these methods excel in specific situations. Fifth, the proposal is to extend the application of these methods into diverse areas, such as developing rules for systems and experts.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
